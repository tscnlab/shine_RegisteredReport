---
title: "data_simulation"
author: "Carolina Guidolin"
date: "2025-05-23"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
```


# Using data from Fernandez-Alonso et al (2025) to create a set of plasuible distribution values for the parameters in our model

Model of interest to test effects of hormones on melatonin suppression:
Melatonin suppression = E2 levels + P4 levels + (E2 + P4 | PID),
where E2 = estradiol levels, P4 = progesterone levels, and PID = participant ID.

The following steps are performed in the code below:
1. Import melatonin suppression data (32 observations, each from a different participant) and summarise melatonin suppression values in this chort. Note that participants in this dataset mostly did not have dilated pupil (col: pupil_dilation), whereas participants in our experiment will all have dilated pupils. However, they received the same light stimulus intensity and wavelength as the participants in our prospective study, delivered with the same methodology (VR headsets). We believe this is the best approximation for the melatonin suppression levels we will observe in our study, and hence a valid dataset to base of data simulation on.
2. Standardise melatonin suppression values so that they are on a scale from -1 to 1
3. Define a list of potential distribution for intercept (standardised scale from -1 to 1)
4. Define a list of potential distribution for the slope of E2 (first predictor) - mean and SD (standardised scale from -1 to 1)
3. Define a list of potential distribution for the slope of P4 (second predictor) - mean and SD (standardised scale from -1 to 1)
4. Define a range for the values of E2 and P4 (standardised scale from -1 to 1)
5. Define noise distribution with mean 0 and SD 0.3
6. For each combination of the distribution and values of E2 and P4, simulate data y (i.e. melatonin suppression) for that model - for the same combination do this 100 times, meaning for each combination you get 100 values of y
7. Calculate the mean and SD of the simulated data of this one y 
8. Compare to reference data and accept values falling within the first and third quantile of the reference data. These values will then be saved and selected for bfda simulations.

## First, we import data about melatonin suppression values from [Fernandez-Alonso(2025)](https://www.biorxiv.org/content/10.1101/2025.01.15.633125v2.full)

We use this data as reference data to understand the range of possible melatonin suppression values given a light exposure of ~90 melanopic EDI (here condition = "bright light"). This experiment was performed with virtually the same protocol as the one we are designing, i.e. light exposure of same intensity presented at the same circadian time and using the same VR headset system. While most participants in this study did not have a dilated pupil (and in our study all participants will have dilated pupil during light exposure), we still consider these findings to be informative for our sample size calculation. 

```{r}
library(tidyverse)

# Importing data
mlt_data <- read.csv("VR_paper_melatonin_results.csv")

# Calculating summary statistics on raw data
mlt_raw_data_summary <- mlt_data %>%
  summarise(mean = mean(mel_supp),
            sd = sd(mel_supp),
            median = median(mel_supp),
            iqr = IQR(mel_supp),
            q1 = quantile(mel_supp, 0.25),
            q3 = quantile(mel_supp, 0.75))

# Standardise raw data so that it falls between -1 and 1
mlt_data_standardised <- mlt_data %>%
  mutate(mel_supp_standardised = (mel_supp - mean(mel_supp))/sd(mel_supp)
  )

## QUESTION! The data above is kind of standardised from -1 to 1 but not precisely, i.e. some values are slightly above -1 and 1. Should this formula (mix max scaling) be used instead?
# mutate(mel_supp_minmax = 2 * ((mel_supp - min(mel_supp)) / (max(mel_supp) - min(mel_supp)) - 0.5))


# Calculating summary statistics on standardised data 
mlt_data_standardised_summary <- mlt_data_standardised %>%
  summarise(mean = mean(mel_supp_standardised),
            sd = sd(mel_supp_standardised),
            median = median(mel_supp_standardised),
            q1 = quantile(mel_supp_standardised, 0.25),
            q3 = quantile(mel_supp_standardised, 0.75)
            )

```

## Given our model of interest, we now start to assign values to the variables in the model
We want to assign a lot of possible values to our predictors, and check how this influences the predicted variable y (melatonin suppression). The goal is to identify a set of predictor values which lead to physiologically-relevant y values, i.e. in line with what observed in the data we already have. Note that we do need to have a range of possible values for E2 and P4. Again, we express these using a standardised scale here (-1 to 1). 

```{r}
# Intercept in standardised scale (since now melatonin data does not go from 0 to 100)
intercept_mean_list <- seq(-1, 1, by = 0.3)

#E2 slope values in standardised scale
e2_slope_mean_list <- seq(-0.5, 0.5, by = 0.1)

#P4 slope values in standardised scale
p4_slope_mean_list <- seq(-0.5, 0.5, by = 0.1)

# E2 and P4 values (expressed as standardised values between -1 and 1)
e2_min_value <- -1
e2_max_value <- 1
p4_min_value <- -1
p4_max_value <- 1

# Number of simulations
num_simulations <- 100

# Extract known mlt values from mlt_data_standardised_summary
mlt_data_standardised_q1 <- mlt_data_standardised_summary$q1
mlt_data_standardised_q3 <- mlt_data_standardised_summary$q3
```

## Simulate data (each combination repeated 100 times)
```{r}
set.seed(20250602)

# Create empty data frame to store results
simulated_data_results <- data.frame(intercept_mean = numeric(),
                                e2_slope_mean = numeric(),
                                p4_slope_mean = numeric(),
                                mean_y = numeric(),
                                sd_y = numeric(),
                                accepted = factor())

for (intercept_mean in intercept_mean_list) { # loop over possible intercept means
  for (e2_slope_mean in e2_slope_mean_list) { # loop over possible e2 slope means
    for (p4_slope_mean in p4_slope_mean_list) { # loop over possible p4 slope means
      
      # Simulate 100 samples of y for each parameter combination
      y <- numeric(num_simulations)
      for (simulation in seq_len(num_simulations)) {
        
        # Sample E2 and P4 from uniform distribution
        e2_value <- runif(1, e2_min_value, e2_max_value)
        p4_value <- runif(1, p4_min_value, p4_max_value)
        
        # Noise ~ N(0, sd = 0.3)
        noise <- rnorm(1, mean = 0, sd = 0.3)
        
        # Intercept ~ N(intercept_mean, sd = 0.2)
        intercept <- rnorm(1, mean = intercept_mean, sd = 0.2)
        
        # Slopes ~ N(slope_mean, sd = 0.2)
        e2_slope <- rnorm(1, mean = e2_slope_mean, sd = 0.2)
        p4_slope <- rnorm(1, mean = p4_slope_mean, sd = 0.2)
        
        
        # Model
        y[simulation] <- intercept + e2_slope*e2_value + p4_slope*p4_value + noise
      
        }
    
    # Calculate summary stats of y
    mean_y <- mean(y)
    sd_y <- sd(y)
      
    # Determine if the mean of y is within q1 and q3 of the known melatonin data 
    accepted <- ifelse(mean_y >= mlt_data_standardised_q1 & mean_y <= mlt_data_standardised_q3, "yes", "no")
      
    # Store results
    simulated_data_results <- rbind(simulated_data_results,
                               data.frame(intercept_mean = intercept_mean,
                                          e2_slope_mean = e2_slope_mean,
                                          p4_slope_mean = p4_slope_mean,
                                          mean_y = mean_y,
                                          sd_y = sd_y,
                                          accepted = factor(accepted)))
    }
  }
}

# Filter dataset to only accept values within mlt_data q1 and q3
accepted_params <- simulated_data_results %>%
  filter(accepted == "yes")

# Summary of accepted paramters
accepted_params_summary <- accepted_params %>%
  summarise(min_intercept = min(intercept_mean),
            max_intercept = max(intercept_mean),
            min_e2 = min(e2_slope_mean),
            median_e2 = median(e2_slope_mean),
            max_e2 = max(e2_slope_mean),
            min_p4 = min(p4_slope_mean),
            median_p4 = median(p4_slope_mean),
            max_p4 = max(p4_slope_mean))

# This should lead to 468 accepted observations. QUESTION! Are we iterating over the same combinations of parameters multiple times "by accident", since the values for the e2 and p4 slopes are the same (-0.5 to 0.5)? Is this a problem?
# Check using 
# any(duplicated(accepted_params)) - yields FALSE
```

## Visualise relationships between simulated variables for intercept and slopes 
```{r}
GGally::ggpairs(simulated_data_results[simulated_data_results$accepted == "yes", 
                                    c("intercept_mean", "e2_slope_mean", "p4_slope_mean")])

```
## Other visualisation, for each parameter of interest
```{r}
library(ggplot2)

ggplot(accepted_params, aes(x = intercept_mean)) +
  geom_histogram(binwidth = 5, fill = "skyblue", color = "black") +
  theme_minimal()

ggplot(accepted_params, aes(x = e2_slope_mean)) +
  geom_histogram(binwidth = 0.1, fill = "salmon", color = "black") +
  theme_minimal()

ggplot(accepted_params, aes(x = p4_slope_mean)) +
  geom_histogram(binwidth = 0.1, fill = "lightgreen", color = "black") +
  theme_minimal()
```
We now have a list of parameters for the intercept, the slope of E2 and the slope of P4 which we know yield values of y (melatonin suppression) that match the melatonin suppression data we have. Note that these values are calculated for standardised E2, P4 and melatonin suppression data. We now want to save these "accepted" values to perform BFDA on them. 

## Save data in a csv which we can import in another script for the bfda
```{r}
write.csv(accepted_params, "informed_parameters.csv", row.names = FALSE)
```

