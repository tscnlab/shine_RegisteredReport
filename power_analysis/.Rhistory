data.frame(
intercept_mean = informed_parameters$intercept_mean[row],
e2_slope_mean = informed_parameters$e2_slope_mean[row],
p4_slope_mean = informed_parameters$p4_slope_mean[row],
TPR = TPR)
)
}
View(bfda_all_bfs)
View(bfda_simulated_data)
library(lmerTest)
library(lme4)
# Run diagnostics on all simulated datasets
model_diagnostics <- lapply(names(all_sim_data), function(name) {
dat <- all_sim_data[[name]]
fit <- tryCatch(
lmer(y ~ e2_value + p4_value + (1 + e2_value + p4_value | id), data = dat),
error = function(e) return(list(error = e$message, name = name))
)
if (inherits(fit, "list")) {
return(fit)  # return error list with name
}
# Extract summary and diagnostics
list(
name = name,
is_singular = isSingular(fit),
convergence_warnings = fit@optinfo$conv$lme4$messages,
summary = summary(fit)
)
})
# Optional: convert to data.frame for viewing
diagnostic_overview <- do.call(rbind, lapply(model_diagnostics, function(x) {
if (!is.list(x) || !"name" %in% names(x)) return(NULL)
data.frame(
sim_name = x$name,
is_singular = ifelse("is_singular" %in% names(x), x$is_singular, NA),
has_convergence_warning = ifelse("convergence_warnings" %in% names(x) && !is.null(x$convergence_warnings), TRUE, FALSE),
error = ifelse("error" %in% names(x), x$error, NA),
stringsAsFactors = FALSE
)
}))
View(diagnostic_overview)
View(model_diagnostics)
summary(
lmerTest::lmer(
y ~ e2_value + p4_value + (1 + e2_value + p4_value | id),
data = all_sim_data[["row1_sim1"]]
)
)
# Checking how many models managed to converge
diagnostic_overview_summary <- diagnostic_overview %>%
summarise(is_singular == TRUE)
library(tidyverse)
# Checking how many models managed to converge
diagnostic_overview_summary <- diagnostic_overview %>%
summarise(is_singular == TRUE)
View(diagnostic_overview_summary)
# Checking how many models managed to converge
diagnostic_overview_summary <- diagnostic_overview %>%
summary(is_singular == TRUE)
View(mlt_raw_data_summary)
View(mlt_data_standardised_summary)
# Checking how many models managed to converge
diagnostic_overview_summary <- diagnostic_overview %>%
summary(is_singular == TRUE)
# Checking how many models managed to converge
diagnostic_overview %>%
summarise(
num_singular = sum(is_singular, na.rm = TRUE),
num_with_convergence_warning = sum(has_convergence_warning, na.rm = TRUE),
num_with_error = sum(!is.na(error))
)
# Checking how many models managed to converge
diagnostic_overview %>%
summarise(
num_singular = sum(!is_singular, na.rm = TRUE),
num_with_convergence_warning = sum(has_convergence_warning, na.rm = TRUE),
num_with_error = sum(!is.na(error))
)
# Checking how many models managed to converge
diagnostic_overview %>%
summarise(
num_singular = sum(is_singular, na.rm = TRUE),
num_with_convergence_warning = sum(has_convergence_warning, na.rm = TRUE),
num_with_error = sum(!is.na(error))
)
model1 <- y ~ e2_value + p4_value + (1 + e2_value + p4_value | id)
diag_model1 <- run_model_diagnostics(model1, all_sim_data)
run_model_diagnostics <- function(model_formula, data_list) {
lapply(names(data_list), function(name) {
dat <- data_list[[name]]
fit <- tryCatch(
lmer(model_formula, data = dat),
error = function(e) return(list(error = e$message, name = name))
)
if (inherits(fit, "list")) {
return(fit)  # error info
}
list(
name = name,
is_singular = isSingular(fit),
convergence_warnings = fit@optinfo$conv$lme4$messages,
summary = summary(fit)
)
})
}
model1 <- y ~ e2_value + p4_value + (1 + e2_value + p4_value | id)
diag_model1 <- run_model_diagnostics(model1, all_sim_data)
model_maximal <- y ~ e2_value + p4_value + (1 + e2_value + p4_value | id)
model_maximal <- y ~ e2_value + p4_value + (1 + e2_value + p4_value | id)
# Run diagnostics for model_maximal
diag_model1 <- run_model_diagnostics(model_maximal, all_sim_data)
# Turning the model diagnostics into a dataframe
diagnostic_overview <- do.call(rbind, lapply(model_diagnostics, function(x) {
if (!is.list(x) || !"name" %in% names(x)) return(NULL)
data.frame(
sim_name = x$name,
is_singular = ifelse("is_singular" %in% names(x), x$is_singular, NA),
has_convergence_warning = ifelse("convergence_warnings" %in% names(x) && !is.null(x$convergence_warnings), TRUE, FALSE),
error = ifelse("error" %in% names(x), x$error, NA),
stringsAsFactors = FALSE
)
}))
# Checking how many models managed to converge
diagnostic_overview %>%
summarise(
num_singular = sum(is_singular, na.rm = TRUE),
num_with_convergence_warning = sum(has_convergence_warning, na.rm = TRUE),
num_with_error = sum(!is.na(error))
)
# Turning the model diagnostics into a dataframe
diagnostic_overview <- do.call(rbind, lapply(diag_model1, function(x) {
if (!is.list(x) || !"name" %in% names(x)) return(NULL)
data.frame(
sim_name = x$name,
is_singular = ifelse("is_singular" %in% names(x), x$is_singular, NA),
has_convergence_warning = ifelse("convergence_warnings" %in% names(x) && !is.null(x$convergence_warnings), TRUE, FALSE),
error = ifelse("error" %in% names(x), x$error, NA),
stringsAsFactors = FALSE
)
}))
# Checking how many models managed to converge
diagnostic_overview %>%
summarise(
num_singular = sum(is_singular, na.rm = TRUE),
num_with_convergence_warning = sum(has_convergence_warning, na.rm = TRUE),
num_with_error = sum(!is.na(error))
)
# Run diagnostics for model_maximal
diag_model_maximal <- run_model_diagnostics(model_maximal, all_sim_data)
# Turning the model diagnostics into a dataframe
diagnostic_overview_maximal <- do.call(rbind, lapply(diag_model_maximal, function(x) {
if (!is.list(x) || !"name" %in% names(x)) return(NULL)
data.frame(
sim_name = x$name,
is_singular = ifelse("is_singular" %in% names(x), x$is_singular, NA),
has_convergence_warning = ifelse("convergence_warnings" %in% names(x) && !is.null(x$convergence_warnings), TRUE, FALSE),
error = ifelse("error" %in% names(x), x$error, NA),
stringsAsFactors = FALSE
)
}))
# Checking how many models managed to converge
diagnostic_overview %>%
summarise(
num_singular = sum(is_singular, na.rm = TRUE),
num_with_convergence_warning = sum(has_convergence_warning, na.rm = TRUE),
num_with_error = sum(!is.na(error))
)
# Checking how many models managed to converge
diagnostic_overview_maximal %>%
summarise(
num_singular = sum(is_singular, na.rm = TRUE),
num_with_convergence_warning = sum(has_convergence_warning, na.rm = TRUE),
num_with_error = sum(!is.na(error))
)
View(mlt_data)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
# Importing data
mlt_data <- read.csv("VR_paper_melatonin_results.csv")
# Calculating summary statistics on raw data
mlt_raw_data_summary <- mlt_data %>%
summarise(mean = mean(mel_supp),
sd = sd(mel_supp),
median = median(mel_supp),
iqr = IQR(mel_supp),
q1 = quantile(mel_supp, 0.25),
q3 = quantile(mel_supp, 0.75))
# Standardise raw data so that it falls between -1 and 1
mlt_data_standardised <- mlt_data %>%
mutate(mel_supp_standardised = (mel_supp - mean(mel_supp))/sd(mel_supp)
)
# Calculating summary statistics on standardised data
mlt_data_standardised_summary <- mlt_data_standardised %>%
summarise(mean = mean(mel_supp_standardised),
sd = sd(mel_supp_standardised),
median = median(mel_supp_standardised),
q1 = quantile(mel_supp_standardised, 0.25),
q3 = quantile(mel_supp_standardised, 0.75)
)
library(tidyverse)
# Importing data
mlt_data <- read.csv("VR_paper_melatonin_results.csv")
# Calculating summary statistics on raw data
mlt_raw_data_summary <- mlt_data %>%
summarise(mean = mean(mel_supp),
sd = sd(mel_supp),
median = median(mel_supp),
iqr = IQR(mel_supp),
q1 = quantile(mel_supp, 0.25),
q3 = quantile(mel_supp, 0.75))
# Standardise raw data so that it falls between -1 and 1
mlt_data_standardised <- mlt_data %>%
mutate(mel_supp_standardised = (mel_supp - mean(mel_supp))/sd(mel_supp)
)
# Calculating summary statistics on standardised data
mlt_data_standardised_summary <- mlt_data_standardised %>%
summarise(mean = mean(mel_supp_standardised),
sd = sd(mel_supp_standardised),
median = median(mel_supp_standardised),
q1 = quantile(mel_supp_standardised, 0.25),
q3 = quantile(mel_supp_standardised, 0.75)
)
View(mlt_data_standardised)
View(mlt_data_standardised)
library(tidyverse)
# Importing data
mlt_data <- read.csv("VR_paper_melatonin_results.csv")
# Calculating summary statistics on raw data
mlt_raw_data_summary <- mlt_data %>%
summarise(mean = mean(mel_supp),
sd = sd(mel_supp),
median = median(mel_supp),
iqr = IQR(mel_supp),
q1 = quantile(mel_supp, 0.25),
q3 = quantile(mel_supp, 0.75))
# Standardise raw data so that it falls between -1 and 1
mlt_data_standardised <- mlt_data %>%
mutate(mel_supp_standardised = (mel_supp - mean(mel_supp))/sd(mel_supp)
)
## QUESTION! The data above is kind of standardised from -1 to 1 but not precisely, i.e. some values are slightly above -1 and 1. Should this formula (mix max scaling) be used instead?
# mutate(mel_supp_minmax = 2 * ((mel_supp - min(mel_supp)) / (max(mel_supp) - min(mel_supp)) - 0.5))
# Calculating summary statistics on standardised data
mlt_data_standardised_summary <- mlt_data_standardised %>%
summarise(mean = mean(mel_supp_standardised),
sd = sd(mel_supp_standardised),
median = median(mel_supp_standardised),
q1 = quantile(mel_supp_standardised, 0.25),
q3 = quantile(mel_supp_standardised, 0.75)
)
View(mlt_data_standardised_summary)
# Intercept in standardised scale (since now melatonin data does not go from 0 to 100)
intercept_mean_list <- seq(-1, 1, by = 0.3)
#E2 slope values in standardised scale
e2_slope_mean_list <- seq(-0.5, 0.5, by = 0.1)
#P4 slope values in standardised scale
p4_slope_mean_list <- seq(-0.5, 0.5, by = 0.1)
# E2 and P4 values (expressed as standardised values between -1 and 1)
e2_min_value <- -1
e2_max_value <- 1
p4_min_value <- -1
p4_max_value <- 1
# Number of simulations
num_simulations <- 100
# Extract known mlt values from mlt_data_standardised_summary
mlt_data_standardised_q1 <- mlt_data_standardised_summary$q1
mlt_data_standardised_q3 <- mlt_data_standardised_summary$q3
set.seed(20250602)
# Create empty data frame to store results
simulated_data_results <- data.frame(intercept_mean = numeric(),
e2_slope_mean = numeric(),
p4_slope_mean = numeric(),
mean_y = numeric(),
sd_y = numeric(),
accepted = factor())
for (intercept_mean in intercept_mean_list) { # loop over possible intercept means
for (e2_slope_mean in e2_slope_mean_list) { # loop over possible e2 slope means
for (p4_slope_mean in p4_slope_mean_list) { # loop over possible p4 slope means
# Simulate 100 samples of y for each parameter combination
y <- numeric(num_simulations)
for (simulation in seq_len(num_simulations)) {
# Sample E2 and P4 from uniform distribution
e2_value <- runif(1, e2_min_value, e2_max_value)
p4_value <- runif(1, p4_min_value, p4_max_value)
# Noise ~ N(0, sd = 0.3)
noise <- rnorm(1, mean = 0, sd = 0.3)
# Intercept ~ N(intercept_mean, sd = 0.2)
intercept <- rnorm(1, mean = intercept_mean, sd = 0.2)
# Slopes ~ N(slope_mean, sd = 0.2)
e2_slope <- rnorm(1, mean = e2_slope_mean, sd = 0.2)
p4_slope <- rnorm(1, mean = p4_slope_mean, sd = 0.2)
# Model
y[simulation] <- intercept + e2_slope*e2_value + p4_slope*p4_value + noise
}
# Calculate summary stats of y
mean_y <- mean(y)
sd_y <- sd(y)
# Determine if the mean of y is within q1 and q3 of the known melatonin data
accepted <- ifelse(mean_y >= mlt_data_standardised_q1 & mean_y <= mlt_data_standardised_q3, "yes", "no")
# Store results
simulated_data_results <- rbind(simulated_data_results,
data.frame(intercept_mean = intercept_mean,
e2_slope_mean = e2_slope_mean,
p4_slope_mean = p4_slope_mean,
mean_y = mean_y,
sd_y = sd_y,
accepted = factor(accepted)))
}
}
}
# Filter dataset to only accept values within mlt_data q1 and q3
accepted_params <- simulated_data_results %>%
filter(accepted == "yes")
# Summary of accepted paramters
accepted_params_summary <- accepted_params %>%
summarise(min_intercept = min(intercept_mean),
max_intercept = max(intercept_mean),
min_e2 = min(e2_slope_mean),
median_e2 = median(e2_slope_mean),
max_e2 = max(e2_slope_mean),
min_p4 = min(p4_slope_mean),
median_p4 = median(p4_slope_mean),
max_p4 = max(p4_slope_mean))
GGally::ggpairs(simulated_data_results[simulated_data_results$accepted == "yes",
c("intercept_mean", "e2_slope_mean", "p4_slope_mean")])
# This should lead to 468 accepted observations. QUESTION! Are we iterating over the same combinations of parameters multiple times "by accident", since the values for the e2 and p4 slopes are the same (-0.5 to 0.5)? Is this a problem?
any(duplicated(accepted_params))
library(ggplot2)
ggplot(accepted_params, aes(x = intercept_mean)) +
geom_histogram(binwidth = 5, fill = "skyblue", color = "black") +
theme_minimal()
ggplot(accepted_params, aes(x = e2_slope_mean)) +
geom_histogram(binwidth = 0.1, fill = "salmon", color = "black") +
theme_minimal()
ggplot(accepted_params, aes(x = p4_slope_mean)) +
geom_histogram(binwidth = 0.1, fill = "lightgreen", color = "black") +
theme_minimal()
write.csv(accepted_params, "informed_parameters.csv", row.names = FALSE)
knitr::opts_chunk$set(echo = TRUE)
# Fixed participant number, dictated by resource limitations
n_ids <- 12
# Select possible values for intercept mean, based on what we know worked from informed_data_simulation
intercept_mean_list <- list(informed_parameters$intercept_mean)
# Select possible values for the slopes (i.e. the betas of the predictors), based on what we know worked from informed_data_simulation
e2_slope_mean_list <- list(informed_parameters$e2_slope_mean)
p4_slope_mean_list <- list(informed_parameters$p4_slope_mean)
# Fix the standard deviations for the intercept and slopes, based on what we know worked
intercept_sd <- 0.2
e2_slope_sd <- 0.2
p4_slope_sd <- 0.2
# Specify values of E2 and P4 (expressed as standardised values between -1 and 1)
e2_min_value <- -1
e2_max_value <- 1
p4_min_value <- -1
p4_max_value <- 1
# Define number of simulations to run
num_simulations <- 10 # should ideally be 1000, but simulation crashed when trying values >10
# Define threshold for Bayes Factor
bf_threshold <- 3
library(tidyverse)
library(lmerTest)
library(BayesFactor)
informed_parameters <- read.csv("informed_parameters.csv")
# Keep only cols of interest
informed_parameters <- informed_parameters %>%
select(intercept_mean, e2_slope_mean, p4_slope_mean)
# Fixed participant number, dictated by resource limitations
n_ids <- 12
# Select possible values for intercept mean, based on what we know worked from informed_data_simulation
intercept_mean_list <- list(informed_parameters$intercept_mean)
# Select possible values for the slopes (i.e. the betas of the predictors), based on what we know worked from informed_data_simulation
e2_slope_mean_list <- list(informed_parameters$e2_slope_mean)
p4_slope_mean_list <- list(informed_parameters$p4_slope_mean)
# Fix the standard deviations for the intercept and slopes, based on what we know worked
intercept_sd <- 0.2
e2_slope_sd <- 0.2
p4_slope_sd <- 0.2
# Specify values of E2 and P4 (expressed as standardised values between -1 and 1)
e2_min_value <- -1
e2_max_value <- 1
p4_min_value <- -1
p4_max_value <- 1
# Define number of simulations to run
num_simulations <- 10 # should ideally be 1000, but simulation crashed when trying values >10
# Define threshold for Bayes Factor
bf_threshold <- 3
# Fixed participant number, dictated by resource limitations
n_ids <- 12
# Select possible values for intercept mean, based on what we know worked from informed_data_simulation
intercept_mean_list <- list(informed_parameters$intercept_mean)
# Select possible values for the slopes (i.e. the betas of the predictors), based on what we know worked from informed_data_simulation
e2_slope_mean_list <- list(informed_parameters$e2_slope_mean)
p4_slope_mean_list <- list(informed_parameters$p4_slope_mean)
# Fix the standard deviations for the intercept and slopes, based on what we know worked
intercept_sd <- 0.2
e2_slope_sd <- 0.2
p4_slope_sd <- 0.2
# Specify values of E2 and P4 (expressed as standardised values between -1 and 1)
e2_min_value <- -1
e2_max_value <- 1
p4_min_value <- -1
p4_max_value <- 1
# Define number of simulations to run
num_simulations <- 5 # should ideally be 1000, but simulation crashed when trying values >10
# Define threshold for Bayes Factor
bf_threshold <- 3
set.seed(20250602)
# Create empty data frame to store simulated data
bfda_simulated_data <- data.frame(intercept_mean = numeric(),
e2_slope_mean = numeric(),
p4_slope_mean = numeric(),
TPR = numeric()
)
# Create an empty dataframe to store all BFs
bfda_all_bfs <- data.frame(
row = numeric(),
simulation = numeric(),
intercept_mean = numeric(),
e2_slope_mean = numeric(),
p4_slope_mean = numeric(),
BF = numeric()
)
# Create a for loop to iterate through the possible combinations of parameters (intercept, e2 slopes, and p4 slope)
for (row in 1:nrow(informed_parameters)) { # looping through each row of the informed_parameter dataframe
# QUESTION! Is this for loop perhaps the reason why it takes so long to run? Should we use expand.grid() instead?
# Store BFs for simulations
bf_list <- numeric(num_simulations)
# Run simulations: for each combination, repeat the simulation n times
for (simulation in seq_len(num_simulations)) {
print(paste0("Row:", row))
print(paste0("Simulation:", simulation))
# Sample intercept and slopes from normal distribution of defined parameters
intercept <- rnorm(n_ids, mean = informed_parameters$intercept_mean[row], sd = intercept_sd)
e2_slope <- rnorm(n_ids, mean = informed_parameters$e2_slope_mean[row], sd = e2_slope_sd)
p4_slope <- rnorm(n_ids, mean = informed_parameters$p4_slope_mean[row], sd = p4_slope_sd)
# Simulate data for n_ids individuals
# We are making the assumption that E2 and P4 are independent (for simplicity)
sim_data <- data.frame(
id = factor(rep(1:n_ids, each = 4)),
e2_value = runif((4*n_ids), e2_min_value, e2_max_value),
p4_value = runif((4*n_ids), p4_min_value, p4_max_value)
)
# Create values for y by solving the equation and add noise
sim_data$y <- intercept[sim_data$id] +
e2_slope[sim_data$id]*sim_data$e2_value + # e2 slope * e2 value for given id
p4_slope[sim_data$id]*sim_data$p4_value + # p4 slope * p4 value for given id
rnorm(n_ids*4,mean = 0, sd = 0.3) # noise ~ N(0, sd=0.3), for each experiment
# Check for absence of NAs in the dataset
if (anyNA(sim_data)) {
print(sim_data[!complete.cases(sim_data), ])
stop(sprintf("NA found in sim_data at row %d, simulation %d", row, simulation))
}
print(paste0("Sim data row",nrow(sim_data))) # print number of rows for sim_data
# Compute Bayes Factor for full model, with predictors being e2 and p4 levels
bf_full <- BayesFactor::lmBF(y ~ e2_value + p4_value + id,
data = sim_data,
whichRandom = "id",
progress = FALSE) # fitting individual intercept for each id
# This function already calculates the ratio between the full model and a model where the intercept is the grand mean
# Compute the Bayes Factor for the null model, with predictor being the id variation
bf_only_intercept <- BayesFactor::lmBF(y ~ id ,
data = sim_data,
whichRandom = "id",
progress = FALSE) # fitting a different intercept for each id
# This function calculates the ratio between a model where the intercept is different for each id compared to a model where the intercept in the grand mean
# Take ratio of these two models, meaning the models where intercept is the grand mean cancel each other out
# So we are effectively taking a ratio between the full model and a model where the intercept is different for id
bf_ratio <- bf_full/bf_only_intercept
print(bf_full)
print(bf_only_intercept)
print(bf_ratio)
# Extract BF value from bf_ratio
bf_numeric <- as.numeric(BayesFactor::extractBF(bf_ratio)$bf)
bf_list[simulation] <- bf_numeric
}
# Compute True Positive Rate (TPR)
TPR <- sum(bf_list > bf_threshold) / num_simulations
# Store in results
bfda_simulated_data <- rbind(bfda_simulated_data,
data.frame(
intercept_mean = informed_parameters$intercept_mean[row],
e2_slope_mean = informed_parameters$e2_slope_mean[row],
p4_slope_mean = informed_parameters$p4_slope_mean[row],
TPR = TPR)
)
}
# Plot heatmap
library(scales)
ggplot(bfda_simulated_data, aes(x = e2_slope_mean, y = p4_slope_mean)) +
geom_tile(aes(fill = TPR), colour = "black") +
scale_fill_gradient(low = "white", high = "darkgreen", name = "TPR") +
labs(
title = "True positive rate (TPR) for E2 and P4 slope means combinations",
x = "E2 slope mean",
y = "P4 slope mean"
) +
scale_x_continuous(
breaks = seq(-0.5, 0.5, by = 0.1),
limits = c(-0.5, 0.5)
) +
scale_y_continuous(
breaks = seq(-0.5, 0.5, by = 0.1),
limits = c(-0.5, 0.5)
) +
coord_fixed(ratio = 1) +
theme_bw() +
theme(
aspect.ratio = 1,
panel.grid = element_blank(),  # clean up grid lines inside tiles
axis.text = element_text(size = 10),
axis.title = element_text(size = 12)
)
# Question -- since the values we are iterating over (means of e2 and p4 slopes are the same, i.e. range from -0.5 to 0.5, does that mean that we have rows that are the same, i.e. are we iterating multiple times over the same combination? If so, is that a problem?)
